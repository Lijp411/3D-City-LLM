import argparse
import os
import io
import sys
import openai
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, BitsAndBytesConfig, CLIPImageProcessor
import base64

sys.path.append("./City-LLM")
from lib.config import CONF
# import gradio as gr
import plotly.graph_objects as go
import json
import rasterio
import time
import re
from PIL import Image
from openai import OpenAI

def get_client_DeepSeek():
    return OpenAI(
        api_key="...", base_url="https://api.deepseek.com")

def main(args):

    client = get_client_DeepSeek()

    def encode_image(image_path):
      with open(image_path, "rb") as image_file:
        print(image_file)
    
        return base64.b64encode(image_file.read()).decode('utf-8')

    system_prompt_self = "You are a fair and impartial expert evaluator responsible for scoring the logicality and reliability of the answers generated by the 3D large language models (on a scale of 0 to 10, with decimals allowed). "

    prompt_base_self = """
    (1) Logicality measures the internal coherence and reasoning validity of a generated response. A logically strong answer should demonstrate a clear argumentative flow, avoid contradictions, and present conclusions that follow consistently from the given premises, regardless of lexical overlap with the reference.
    (2) Reliability indicates the factual correctness and evidence alignment of a response. A reliable answer must faithfully reflect the provided ground-truth information and scene evidence, avoiding hallucinated details or unsupported claims, thereby ensuring trustworthiness.
    """

    prompt_final_self = """The output must strictly adhere to the following format (where X.X represents a numerical score between 0 and 10, with one decimal place allowed; provide concise justification after rating, No json format):
    {
    Answer Logicality: X.X
    Answer Reliability: X.X
    Reason: ...
    }"""


    SCANREFER_TRAIN = json.load(open("./TestResults/Object_Caption_Test.json")) # TODO: change this

    SCANREFER_VAL = json.load(open(".TestResults/Object_Caption_Test.json")) # TODO: change this

    file_path = ".TestResults/ChatGPT_Object_Caption_Test.txt" # TODO: change this

    scanrefer_train, scanrefer_val, all_scene_list= get_scanrefer(SCANREFER_TRAIN, SCANREFER_VAL, -1)
    scanrefer = {"train": scanrefer_train, "val": scanrefer_val}
    scanrefer = scanrefer["val"]
    output_dict = []
    K = 5
    Fine_grained_building_category = {'Commercial': 0, 'Residential': 1, 'Office': 2, 'Cultural': 3, 'Transportation': 4, 'Municipal': 5, 'Temporary': 6, 'Unclassified': 7}
    print("The total val sample is: ",len(scanrefer))

    for i in range(len(scanrefer)):

        start_time = time.time()

        idx = i
        print(idx)

        scene_inf = scanrefer[idx]["scene_id"]
        obj_id = scanrefer[idx].get("object_id", None)
        try:
            obj_id = int(obj_id)
        except (TypeError, ValueError):
            obj_id = "-"

        task_type = scanrefer[idx]["type"]
        question = scanrefer[idx]["question"]
        GT_answer = scanrefer[idx]["answer"]
        Pred_answer = scanrefer[idx]["pred"]

        prompt_question = "The question is: <Question-Start> " + question + "<Question-End>. " + "The ground truth answer is: <Start-Answer-GT> " + GT_answer + "<End-Answer-GT>. "

        prompt_answer = "The Answer is: <Start-Answer> " + Pred_answer + "<End-Answer>. "

        if obj_id != "-":
          object_id = int(obj_id)
        else:
          object_id = None

        object_prompt = ""
        landmark_prompt = ""

        scene_parts = scene_inf.split("-", 1)
        if len(scene_parts) == 2:
            scene_name, scene_id = scene_parts

        local_image = np.zeros((3, 9, 9))
        global_image = np.zeros((3, 9, 9))
        lang_gt = 1
        pc_feats = np.zeros((1, 1024))
        landmark_feats = np.zeros((1, 768))
        pc_feats_nearest = np.zeros((5, 1024))
        landmark_feats_nearest = np.zeros((5, 768))
        

        if scene_name == "SensatUrban":

          pg_file = os.path.join(CONF.PATH.SensatUrban_PG, scene_id + ".pth")
          feat_file = os.path.join(CONF.PATH.SensatUrban_FEAT, scene_id+".json")
          landmark_file = os.path.join(CONF.PATH.SensatUrban_LANDMARK, scene_id+".landmark_feat.json")
          landmark_nearest_file = os.path.join(CONF.PATH.SensatUrban_LANDMARK, scene_id+".landmark_feat.json")
          landmark_name_file = os.path.join(CONF.PATH.SensatUrban_BOX, scene_id+"_bbox.json")

          coords, colors, label_ids, instance_ids, label_ids_pg, instance_ids_pg, instance_bboxes, \
              landmark_names, landmark_ids, globalShift = torch.load(pg_file, weights_only = False)
            
          with open(feat_file, 'r') as json_file:
              feats = json.load(json_file)
            
          with open(landmark_file, 'r') as json_file:
              landmark_feat = json.load(json_file)

          with open(landmark_nearest_file, 'r') as json_file:
              landmark_nearest_feat = json.load(json_file)

          with open(landmark_name_file, 'r') as json_file:
              landmark_name_idx_file = json.load(json_file)

          mesh_vertices = np.concatenate([coords, colors], axis=1) 
          instance_bboxes = np.stack([instance_bboxes[instance_id] for instance_id in sorted(instance_bboxes.keys()) if instance_id != -100])

          instance_labels = instance_ids_pg
          semantic_labels = label_ids_pg

          point_cloud = mesh_vertices[:, 0:6]
          point_cloud[:, 3:6] = point_cloud[:, 3:6] / 127.5 - 1 

          floor_height = np.percentile(point_cloud[:, 2], 0.99)
          height = point_cloud[:, 2] - floor_height
          point_cloud = np.concatenate([point_cloud, np.expand_dims(height, 1)], 1)

          instance_class = []
          pc_feats = []

          cand_instance_ids = [cand_id for cand_id in np.unique(instance_labels) if cand_id != -100]
          nearest_instance_ids = find_nearest_instances(instance_bboxes, object_id, 5) # Finding nearest instances
          nearest_instance_landmark_ids = find_nearest_instances(instance_bboxes, object_id, 5) # Finding nearest instances
        
          ## For Object-level and Relationship-level Tasks
          item_id = object_id

          # Image - Top view feat
          raster_file = os.path.join(CONF.PATH.SensatUrban_MAP, scene_id+".tif")
          image = safe_read_raster(raster_file)
      
          instance_bboxes_pixel = np.copy(instance_bboxes)
          instance_bboxes_pixel[:,0] = instance_bboxes_pixel[:,0] / 0.1
          instance_bboxes_pixel[:,1] = instance_bboxes_pixel[:,1] / 0.1
          instance_bboxes_pixel[:,3] = instance_bboxes_pixel[:,3] / 0.1
          instance_bboxes_pixel[:,4] = instance_bboxes_pixel[:,4] / 0.1
          arr = instance_bboxes[:,-1]
          row = find_rows_with_value(arr, item_id)

          if len(row) == 0:
            subimage = np.zeros((256,256,3))
          else:         
            X_pixel = int(instance_bboxes_pixel[row, 0])
            Y_pixel = int(instance_bboxes_pixel[row, 1])
            X_delta = int(instance_bboxes_pixel[row, 3])
            Y_delta = int(instance_bboxes_pixel[row, 4])

            bbox = instance_bboxes[row, :]
            bbox = np.array(bbox).reshape(-1)
            x, y, z = bbox[0], bbox[1], bbox[2]
            dx, dy, dz = bbox[3], bbox[4], bbox[5]

            object_prompt = (
              f"The object is located at coordinates "
              f"({x:.2f}, {y:.2f}, {z:.2f}) meters, "
              f"with spatial spans of {dx:.2f} meters along X, "
              f"{dy:.2f} meters along Y, and {dz:.2f} meters along Z."
            )

            Image_LH = max(X_delta, Y_delta)

            image_with_rectangle = image[:, :, :]
            x1 = int(image.shape[2] - Y_pixel - int(Y_delta/2))
            y1 = int(X_pixel - int(X_delta/2))
            x2 = int(image.shape[2] - Y_pixel + int(Y_delta/2))
            y2 = int(X_pixel + int(X_delta/2))
            thickness = min(10, x2 - x1, y2 - y1)

            if min(10, x2 - x1, y2 - y1) < 100:
              Image_LH = 10 * max(X_delta, Y_delta)
              thickness = 2
            
            for t in range(thickness):
              try:
                image_with_rectangle = image[:, :, :]
                image_with_rectangle[0, x1:x2, y1 + t + 1] = 255
                image_with_rectangle[0, x1:x2, y2 - t - 1] = 255
                image_with_rectangle[0, x1 + t + 1, y1:y2] = 255
                image_with_rectangle[0, x2 - t - 1, y1:y2] = 255
                image_with_rectangle[1, x1:x2, y1 + t + 1] = 0
                image_with_rectangle[1, x1:x2, y2 - t - 1] = 0
                image_with_rectangle[1, x1 + t + 1, y1:y2] = 0
                image_with_rectangle[1, x2 - t - 1, y1:y2] = 0
                image_with_rectangle[2, x1:x2, y1 + t + 1] = 0
                image_with_rectangle[2, x1:x2, y2 - t - 1] = 0
                image_with_rectangle[2, x1 + t + 1, y1:y2] = 0
                image_with_rectangle[2, x2 - t - 1, y1:y2] = 0
              except IndexError:
                continue
            
            local_image = extract_subimage(image_with_rectangle, image_with_rectangle.shape[2] - Y_pixel, X_pixel, Image_LH, Image_LH)
        
          local_image = np.transpose(local_image, (1, 2, 0))
          global_image = np.transpose(image, (1, 2, 0))
        
          # PC - Uni3d feat
          pc_feats = []
          landmark_feats = []

          try:
            feat_instance = feats[str(int(item_id))]
          except (KeyError, TypeError):
            feat_instance = np.zeros((1, 1024)) # Unable to equip pre-trained pc_feats for object
          pc_feats.append(feat_instance)
          pc_feats = np.vstack(pc_feats)

          # Landmark - BiGRU feat
          try:
            landmark_feat_instance = landmark_feat[str(int(item_id))]
          except (KeyError, TypeError):
            landmark_feat_instance = np.zeros((1, 768)) # Unable to equip pre-trained pc_feats for object
        
          landmark_feats.append(landmark_feat_instance)
          landmark_feats = np.vstack(landmark_feats)

          # K-nearest objects
          while len(nearest_instance_ids) < 5: # K value 
            nearest_instance_ids.append(object_id)

          if isinstance(nearest_instance_landmark_ids, np.ndarray):
            nearest_instance_landmark_ids = nearest_instance_landmark_ids.tolist()

          while len(nearest_instance_landmark_ids) < 5: # K value 
            nearest_instance_landmark_ids.append(object_id)

          # K-nearest objects - Uni3d feat
          pc_feats_nearest = []
          for i in nearest_instance_ids:
            feat_instance = np.zeros((1, 1024))
            try:
                feat_instance = feats[str(int(i))]
            except (KeyError, TypeError):
                feat_instance = np.zeros((1, 1024))
            pc_feats_nearest.append(feat_instance)

          pc_feats_nearest = np.vstack(pc_feats_nearest)

          # K-nearest landmark - BiGRU feat
          landmark_feats_nearest = []
          for i in nearest_instance_ids:
            landmark_feat_instance = np.zeros((1, 768))
            try:
                landmark_feat_instance = landmark_nearest_feat[str(int(i))]
            except (KeyError, TypeError):
                landmark_feat_instance = np.zeros((1, 768))
            landmark_feats_nearest.append(landmark_feat_instance)

          landmark_feats_nearest = np.vstack(landmark_feats_nearest)

          # K-nearest landmark name - tokenizer in llava 
          landmark_names_nearest = []
          for i in nearest_instance_landmark_ids:
            landmark_name_instance = ""
            for bbox in landmark_name_idx_file['bboxes']:
                if int(bbox['object_id']) == i:
                    landmark_name_instance = bbox['landmark']
                break
            landmark_names_nearest.append(landmark_name_instance)

          # additional prompt
          landmark_name_object = None
          for bbox in landmark_name_idx_file['bboxes']:
              if int(bbox['object_id']) == item_id:
                  landmark_name_object = bbox['landmark']
              break
          if landmark_name_object is not None:
            landmark_prompt = "The landmark of this object: " + landmark_name_object + ". "

          if any(landmark_names_nearest):
            non_empty_landmark = [item for item in landmark_names_nearest if item]
            landmark_prompt = ", ".join(non_empty_landmark)
            landmark_prompt = ". The landmark next to this object: " + landmark_prompt + ". "

          # GT
          lang_ans =  []
          for i_instance in cand_instance_ids:
            if (i_instance == object_id) and (i_instance == item_id):
                lang_ans.append(1)
            else:
                lang_ans.append(0)
          lang_gt = 1
          pc_feats = pc_feats[0]
          landmark_feats = landmark_feats[0]

        if scene_name == "UrbanBIS":

          scene_id_converted = scene_id.replace("_", "/", 1)
          feat_file = os.path.join(CONF.PATH.UrbanBIS_FEAT, scene_id_converted + "_feat.json")
          bbox_file = os.path.join(CONF.PATH.UrbanBIS_BOX, scene_id_converted + "_bbox.json")
          pc_file = os.path.join(CONF.PATH.UrbanBIS_Inst, scene_id_converted + ".txt")
          raster_file = os.path.join(CONF.PATH.UrbanBIS_MAP, scene_id_converted + ".tif")
          image = safe_read_raster(raster_file)
          global_image = np.transpose(image, (1, 2, 0))

          with open(feat_file, 'r') as json_file:
            feats = json.load(json_file)

          with open(bbox_file, 'r') as json_file:
            bbox = json.load(json_file)

          point_cloud = np.loadtxt(pc_file)

          instance_bboxes = [] # object instance
          instance_bbox = []
          instance_labels = []

          for i in range(len(bbox['bboxes'])):
            bbox_instance = np.array(bbox['bboxes'][i]['bbox'])
            instance_bboxes.append(bbox_instance)
            if bbox['bboxes'][i]['object_id'] ==  object_id:
                instance_bbox = bbox['bboxes'][i]['bbox']
          
          instance_bboxes = np.stack(instance_bboxes)
          instance_labels = instance_bboxes[:, -1]

          if object_id is None: # scene task: only landmark feat/prompt and global clip
            pc_feats = np.zeros((1, 1024))
            landmark_feats = np.zeros((1, 768))
            pc_feats_nearest = np.zeros((5, 1024))
            landmark_feats_nearest = np.zeros((5, 768))
            local_image = np.transpose(local_image, (1, 2, 0))

            category_ids = point_cloud[:, 8]
            unique_category_ids = np.unique(category_ids)
            id_to_category = {v: k for k, v in Fine_grained_building_category.items()}
            category_names = [id_to_category.get(int(cid), "Ordinary Building") for cid in unique_category_ids]

            if len(category_names) == 1:
              landmark_prompt = f"The scene contains {category_names[0].lower()} buildings."
            else:
              landmark_prompt = "The scene contains " + ", ".join([c.lower() for c in category_names[:-1]]) \
                   + f", and {category_names[-1].lower()} buildings."

            object_prompt = ""

          else: 
            pc_feats = []
            cand_instance_ids = [cand_id for cand_id in np.unique(instance_labels) if cand_id != -100]
            nearest_instance_ids = find_nearest_instances(instance_bboxes, object_id, 5) # Finding nearest instances

            item_id = object_id

            # Image - Top view feat
            min_x = np.min(point_cloud[:, 0])
            min_y = np.min(point_cloud[:, 1])
            max_x = np.max(point_cloud[:, 0])
            max_y = np.max(point_cloud[:, 1])

            X_pixel = int(instance_bbox[0] - min_x) / 0.1
            Y_pixel = int(instance_bbox[1] - min_y) / 0.1
            X_delta = int(instance_bbox[3]) / 0.1
            Y_delta = int(instance_bbox[4]) / 0.1     
            Image_LH = max(X_delta, Y_delta)

            image_with_rectangle = image[:, :, :]
            x1 = int(X_pixel - int(X_delta))
            y1 = int(image.shape[1] - Y_pixel - int(1.0 * Y_delta))

            x2 = int(X_pixel + int(X_delta))
            y2 = int(image.shape[1] - Y_pixel + int(1.0 * Y_delta))
            thickness = min(6, x2 - x1, y2 - y1)

            if min(x2 - x1, y2 - y1) < 100:
              Image_LH = 10 * max(X_delta, Y_delta)
              thickness = 8
        
            if min(x2 - x1, y2 - y1) > 100:
              Image_LH = 10 * max(X_delta, Y_delta)
              thickness = 20

            for t in range(thickness):
              try:
                image_with_rectangle[0, y1:y2, x1 + t + 1] = 255
                image_with_rectangle[0, y1:y2, x2 - t - 1] = 255
                image_with_rectangle[0, y1 + t + 1, x1:x2] = 255
                image_with_rectangle[0, y2 - t - 1, x1:x2] = 255
                image_with_rectangle[1, y1:y2, x1 + t + 1] = 0
                image_with_rectangle[1, y1:y2, x2 - t - 1] = 0
                image_with_rectangle[1, y1 + t + 1, x1:x2] = 0
                image_with_rectangle[1, y2 - t - 1, x1:x2] = 0
                image_with_rectangle[2, y1:y2, x1 + t + 1] = 0
                image_with_rectangle[2, y1:y2, x2 - t - 1] = 0
                image_with_rectangle[2, y1 + t + 1, x1:x2] = 0
                image_with_rectangle[2, y2 - t - 1, x1:x2] = 0
              except IndexError:
                continue

            if min(x2 - x1, y2 - y1) >= 100:
              local_image = extract_subimage_urbanbis(image_with_rectangle, image.shape[1] - Y_pixel, X_pixel, 10 * Y_delta, 10 * X_delta)
            
            if min(x2 - x1, y2 - y1) < 100:
              local_image = extract_subimage_urbanbis(image_with_rectangle, image.shape[1] - Y_pixel, X_pixel, 20 * Y_delta, 20 * X_delta)

            local_image = np.transpose(local_image, (1, 2, 0))

            # PC - Uni3d feat 
            pc_feats = []
            landmark_feats = []
            try:
              feat_instance = feats[str(int(item_id))]
            except (KeyError, TypeError):
              feat_instance = np.zeros((1, 1024)) # Unable to equip pre-trained pc_feats for object
            pc_feats.append(feat_instance)
            pc_feats = np.vstack(pc_feats)

            # Landmark - BiGRU feat
            landmark_feat_instance = np.zeros((1, 768))
            landmark_feats.append(landmark_feat_instance)
            landmark_feats = np.vstack(landmark_feats)
            try:
              while len(nearest_instance_ids) < K: # K value 
                nearest_instance_ids.append(object_id)
            except AttributeError:
              nearest_instance_ids = [object_id] * K

            # Nei. pc
            pc_feats_nearest = []
            for i in nearest_instance_ids:
              feat_instance = np.zeros((1, 1024))
              try:
                feat_instance = feats[str(int(i))]
              except (KeyError, TypeError):
                feat_instance = np.zeros((1, 1024))
              pc_feats_nearest.append(feat_instance)
            pc_feats_nearest = np.vstack(pc_feats_nearest)

            # Nei. landmark
            landmark_feats_nearest = []
            for i in nearest_instance_ids:
              feat_instance = np.zeros((1, 768))
              landmark_feats_nearest.append(feat_instance)
            landmark_feats_nearest = np.vstack(landmark_feats_nearest)

            # additional prompt
            point_cloud_instance = point_cloud[point_cloud[:, 7] == item_id]
            CategoryID = point_cloud_instance[1,8]
            id_to_category = {v: k for k, v in Fine_grained_building_category.items()}
            category = id_to_category.get(CategoryID, "Ordinary Building")
            landmark_prompt = " The class of this building object belongs to '" + category.lower() + "'. "
            object_prompt = ""
            bbox = instance_bbox
            x, y, z = bbox[0], bbox[1], bbox[2]
            dx, dy, dz = bbox[3], bbox[4], bbox[5]

            object_prompt = (
              f"The object is located at coordinates "
              f"({x:.2f}, {y:.2f}, {z:.2f}) meters, "
              f"with spatial spans of {dx:.2f} meters along X, "
              f"{dy:.2f} meters along Y, and {dz:.2f} meters along Z."
            )

            lang_gt = 1


        #### Question and prompt for evaluators
        image = Image.fromarray(local_image)
        image_source = io.BytesIO()
        image.save(image_source, format='PNG')
        image_source.seek(0)
        base64_image = base64.b64encode(image_source.read()).decode('utf-8')
        
        prompt_add_information = "Additional reference information is available: " + object_prompt + landmark_prompt
        prompt_final = prompt_base_self + prompt_final_self + prompt_question + prompt_answer + prompt_add_information

        response = client.chat.completions.create(
          model="deepseek-reason",
          messages=[
            {"role": "system", "content": system_prompt_self},
            {"role": "user", "content": prompt_final},
          ],
          stream=False
        )
        
        outputs_1 = response.choices[0].message.content

        print("The output is: " + outputs_1)

        with open(file_path, 'a', encoding='utf-8') as file:
            file.write("order: " + str(idx) + ", scene_id: " + scene_id + "\n")
            file.write("Score: "+ outputs_1 + "\n")

    print("Completed")


def get_scanrefer(scanrefer_train, scanrefer_val, num_scenes, train_scenes_to_use=None, val_scenes_to_use=None):
    # get initial scene list
    if train_scenes_to_use is not None:
        train_scene_list = sorted(list(set([data["scene_id"] for data in scanrefer_train if data["scene_id"] in train_scenes_to_use])))
    else:
        train_scene_list = sorted(list(set([data["scene_id"] for data in scanrefer_train])))
        
    if val_scenes_to_use is not None:
        val_scene_list = sorted(list(set([data["scene_id"] for data in scanrefer_val if data["scene_id"] in val_scenes_to_use])))
    else:        
        val_scene_list = sorted(list(set([data["scene_id"] for data in scanrefer_val])))
        
    if num_scenes == -1:
        num_scenes = len(train_scene_list)
    else:
        assert len(train_scene_list) >= num_scenes
        
    # slice train_scene_list
    train_scene_list = train_scene_list[:num_scenes]

    # filter data in chosen scenes
    new_scanrefer_train = []
    for data in scanrefer_train:
        if data["scene_id"] in train_scene_list:
            new_scanrefer_train.append(data)

    new_scanrefer_val = []
    for data in scanrefer_val:
        if data["scene_id"] in val_scene_list:
            new_scanrefer_val.append(data)

    # all scanrefer scene
    all_scene_list = train_scene_list + val_scene_list

    print("train on {} samples and val on {} samples".format(len(new_scanrefer_train), len(new_scanrefer_val)))

    return new_scanrefer_train, new_scanrefer_val, all_scene_list


## def for construction of our cityrefer dataset (Query, 3D instance, 2D image)
def one_hot(length, position):
    zeros = [0 for _ in range(length)]
    zeros[position] = 1
    zeros = np.array(zeros)
    return zeros


def shuffle_items_with_indices(lst):
    indices = list(range(len(lst)))  
    random.shuffle(indices) 
    shuffled_items = [lst[idx] for index, idx in enumerate(indices)]
    return shuffled_items, indices


def extract_subimage(image, center_x, center_y, width, height):

    start_x = max(center_x - width // 2, 0)
    end_x = min(center_x + width // 2, image.shape[2])
    start_y = max(center_y - height // 2, 0)
    end_y = min(center_y + height // 2, image.shape[1])
    subimage = image[:,  start_x:end_x,start_y:end_y]
    
    return subimage

def extract_subimage_urbanbis(image, center_x, center_y, width, height):

    start_x = max(center_x - width // 2, 0)
    end_x = min(center_x + width // 2, image.shape[1])
    start_y = max(center_y - height // 2, 0)
    end_y = min(center_y + height // 2, image.shape[2])
    subimage = image[:, int(start_x):int(end_x), int(start_y):int(end_y)]
    
    return subimage

def find_rows_with_value(arr, value):
    rows_with_value = np.where(arr == value)[0]
    return rows_with_value

def find_nearest_instances(arr, objectID,K):
    object_row = arr[arr[:, -1] == objectID]
    if object_row.shape[0] == 0:
        return []
    object_xyz = object_row[:, :3]
    distances = np.sqrt(np.sum((arr[:, :3] - object_xyz) ** 2, axis=1))
    distances[arr[:, -1] == objectID] = np.inf
    nearest_indices = np.argsort(distances)[:K]
    return arr[nearest_indices, -1]


def safe_read_raster(raster_file):
    try:
        with rasterio.open(raster_file) as src:
            image = src.read()
            if image is None or image.size == 0:
                return np.zeros((3, 224, 224), dtype=np.uint8)
            
            if image.shape[0] == 1:
                image = np.repeat(image, 3, axis=0)
            
            if image.shape[1] == 0 or image.shape[2] == 0:
                return np.zeros((3, 224, 224), dtype=np.uint8)
            
            return image
        
    except Exception as e:
        return np.zeros((3, 224, 224), dtype=np.uint8)

if __name__ == "__main__":
    main(sys.argv[1:])
